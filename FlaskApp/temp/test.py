import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import random
from urllib.parse import urljoin
import logging
import re
import os
from threading import Thread
import json

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class AutoDromParser:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.base_url = "https://baza.drom.ru"

    def search_part(self, brand, model, part):
        """–ü–æ–∏—Å–∫ —Ü–µ–Ω—ã –∏ —Å—Å—ã–ª–∫–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –¥–µ—Ç–∞–ª–∏"""
        try:
            search_query = f"{brand} {model} {part}".strip()
            encoded_query = search_query.replace(' ', '+')
            
            search_url = f"{self.base_url}/?query={encoded_query}"
            
            logging.info(f"üîç –ü–æ–∏—Å–∫: {search_query}")
            
            response = self.session.get(search_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            price, link = self.find_price_and_link(soup)
            
            if not link:
                link = search_url
            
            if price == 0:
                logging.warning(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Ü–µ–Ω–∞ –¥–ª—è: {search_query}")
            else:
                logging.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ: {price} —Ä—É–±. –¥–ª—è {part}")
            
            return price, link
                
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è {brand} {model} {part}: {e}")
            return 0, ""

    def find_price_and_link(self, soup):
        """–ü–æ–∏—Å–∫ —Ü–µ–Ω—ã –∏ —Å—Å—ã–ª–∫–∏ –≤ HTML"""
        listings = self.find_listings(soup)
        
        if not listings:
            return 0, None
        
        for listing in listings:
            price, link = self.extract_from_listing(listing)
            if price > 0 and link:
                return price, link
        
        first_listing = listings[0]
        price, link = self.extract_from_listing(first_listing)
        return price, link

    def find_listings(self, soup):
        """–ü–æ–∏—Å–∫ —Å–ø–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
        listing_selectors = [
            'a[data-ftid="bulls-list_bull"]',
            'div[data-ftid="component_bullseye"]',
            '[class*="bull-item"]',
            '[class*="bulla"]',
            '[class*="listing-item"]',
        ]
        
        for selector in listing_selectors:
            listings = soup.select(selector)
            if listings:
                return listings
        return None

    def extract_from_listing(self, listing):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏ —Å—Å—ã–ª–∫–∏ –∏–∑ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        try:
            link = self.extract_link(listing)
            price = self.extract_price_from_listing(listing)
            return price, link
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
            return 0, None

    def extract_link(self, element):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–∫–∏"""
        try:
            if element.name == 'a' and element.get('href'):
                href = element['href']
                if href.startswith('/'):
                    return urljoin(self.base_url, href)
                return href
            
            link_selectors = ['a[href*="/offer/"]', 'a[href*="/s/"]', 'a']
            for selector in link_selectors:
                link_element = element.select_one(selector)
                if link_element and link_element.get('href'):
                    href = link_element['href']
                    if href.startswith('/'):
                        return urljoin(self.base_url, href)
                    return href
            return None
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–∫–∏: {e}")
            return None

    def extract_price_from_listing(self, listing):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã"""
        try:
            price_selectors = [
                '[data-ftid="bull_price"]',
                '.bull-item__price',
                '.bulla__price',
                '*[class*="price"]',
            ]
            
            for selector in price_selectors:
                price_element = listing.select_one(selector)
                if price_element:
                    price_text = price_element.get_text(strip=True)
                    cleaned_price = self.clean_price(price_text)
                    if cleaned_price > 0:
                        return cleaned_price
            
            listing_text = listing.get_text()
            price_patterns = [r'(\d[\d\s]*)\s*(—Ä—É–±|‚ÇΩ|—Ä\.|—Ä—É–±–ª–µ–π)']
            
            for pattern in price_patterns:
                matches = re.findall(pattern, listing_text, re.IGNORECASE)
                if matches:
                    for match in matches:
                        price_text = match[0] if isinstance(match, tuple) else match
                        cleaned_price = self.clean_price(price_text)
                        if cleaned_price > 0:
                            return cleaned_price
            
            return 0
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ü–µ–Ω—ã: {e}")
            return 0

    def clean_price(self, price_text):
        """–û—á–∏—Å—Ç–∫–∞ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ü–µ–Ω—ã"""
        try:
            digits_only = re.sub(r'[^\d]', '', str(price_text))
            if digits_only and len(digits_only) >= 2:
                price_num = int(digits_only)
                if 10 <= price_num <= 10000000:
                    return price_num
            return 0
        except:
            return 0

    def parse_damaged_parts(self, brand, model, damaged_parts):
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π
        
        Args:
            brand (str): –ú–∞—Ä–∫–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è
            model (str): –ú–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–æ–±–∏–ª—è  
            damaged_parts (list): –°–ø–∏—Å–æ–∫ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π
        
        Returns:
            list: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –¥–µ—Ç–∞–ª—è—Ö
        """
        results = []
        
        logging.info(f"üöó –ù–∞—á–∏–Ω–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è {brand} {model}")
        logging.info(f"üîß –ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ –¥–µ—Ç–∞–ª–∏: {damaged_parts}")
        
        for i, part in enumerate(damaged_parts, 1):
            logging.info(f"üì¶ –ü–∞—Ä—Å–∏–Ω–≥ {i}/{len(damaged_parts)}: {part}")
            
            price, link = self.search_part(brand, model, part)
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            area = self.determine_area(part)
            material = self.determine_material(part)
            
            result = {
                '–º–∞—Ä–∫–∞': brand,
                '–º–æ–¥–µ–ª—å': model,
                '–¥–µ—Ç–∞–ª—å': part,
                '–ø–ª–æ—â–∞–¥—å_–¥–µ—Ç–∞–ª–∏': area,
                '–º–∞—Ç–µ—Ä–∏–∞–ª_–¥–µ—Ç–∞–ª–∏': material,
                '—Ü–µ–Ω–∞': price,
                '—Å—Å—ã–ª–∫–∞': link
            }
            
            results.append(result)
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if i < len(damaged_parts):
                delay = random.uniform(1, 3)
                time.sleep(delay)
        
        logging.info(f"‚úÖ –ê–≤—Ç–æ–ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(results)} –¥–µ—Ç–∞–ª–µ–π")
        return results

    def determine_area(self, part):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–∏ –¥–µ—Ç–∞–ª–∏"""
        part_lower = part.lower()
        if any(word in part_lower for word in ['–±–∞–º–ø–µ—Ä', '–¥–≤–µ—Ä—å', '–∫–∞–ø–æ—Ç', '–∫—Ä—ã—à–∫–∞', '–∫—Ä—ã–ª–æ']):
            return "–±–æ–ª—å—à–∞—è"
        elif any(word in part_lower for word in ['—Ñ–∞—Ä–∞', '–∑–µ—Ä–∫–∞–ª–æ', '—Å—Ç–µ–∫–ª–æ']):
            return "—Å—Ä–µ–¥–Ω—è—è" 
        else:
            return "–º–∞–ª–∞—è"

    def determine_material(self, part):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞"""
        part_lower = part.lower()
        if any(word in part_lower for word in ['—Å—Ç–µ–∫–ª–æ', '—Ñ–∞—Ä–∞']):
            return "—Å—Ç–µ–∫–ª–æ"
        elif any(word in part_lower for word in ['–±–∞–º–ø–µ—Ä', '–∑–µ—Ä–∫–∞–ª–æ', '–ø–ª–∞—Å—Ç–∏–∫']):
            return "–ø–ª–∞—Å—Ç–∏–∫"
        else:
            return "–º–µ—Ç–∞–ª–ª"

def auto_parse_damages(brand, model, damaged_parts):
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–∑–æ–≤–∞ –∏–∑ Flask –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    
    Args:
        brand (str): –ú–∞—Ä–∫–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è
        model (str): –ú–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–æ–±–∏–ª—è
        damaged_parts (list): –°–ø–∏—Å–æ–∫ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π
    
    Returns:
        pd.DataFrame: DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
    """
    parser = AutoDromParser()
    results = parser.parse_damaged_parts(brand, model, damaged_parts)
    return pd.DataFrame(results)

def update_excel_with_parsed_data(parsed_df, excel_file='huh_result.xlsx'):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç Excel —Ñ–∞–π–ª —Å –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –ø–∞—Ä—Å–∏–Ω–≥–∞
    
    Args:
        parsed_df (pd.DataFrame): DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        excel_file (str): –ü—É—Ç—å –∫ Excel —Ñ–∞–π–ª—É
    """
    try:
        if os.path.exists(excel_file):
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª
            existing_df = pd.read_excel(excel_file)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∏–ª–∏ –¥–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
            for _, new_row in parsed_df.iterrows():
                mask = (existing_df['–º–∞—Ä–∫–∞'] == new_row['–º–∞—Ä–∫–∞']) & \
                       (existing_df['–º–æ–¥–µ–ª—å'] == new_row['–º–æ–¥–µ–ª—å']) & \
                       (existing_df['–¥–µ—Ç–∞–ª—å'] == new_row['–¥–µ—Ç–∞–ª—å'])
                
                if mask.any():
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å
                    idx = mask.idxmax()
                    existing_df.loc[idx, '—Ü–µ–Ω–∞'] = new_row['—Ü–µ–Ω–∞']
                    existing_df.loc[idx, '—Å—Å—ã–ª–∫–∞'] = new_row['—Å—Å—ã–ª–∫–∞']
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
                    existing_df = pd.concat([existing_df, new_row.to_frame().T], ignore_index=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            existing_df.to_excel(excel_file, index=False)
            logging.info(f"‚úÖ Excel —Ñ–∞–π–ª –æ–±–Ω–æ–≤–ª–µ–Ω: {excel_file}")
        else:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª
            parsed_df.to_excel(excel_file, index=False)
            logging.info(f"‚úÖ –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π Excel —Ñ–∞–π–ª: {excel_file}")
            
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è Excel: {e}")

def start_auto_parsing(brand, model, damaged_parts, callback=None):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    
    Args:
        brand (str): –ú–∞—Ä–∫–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è
        model (str): –ú–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–æ–±–∏–ª—è
        damaged_parts (list): –°–ø–∏—Å–æ–∫ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π
        callback (function): –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
    """
    def parsing_thread():
        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥
            parsed_df = auto_parse_damages(brand, model, damaged_parts)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º Excel —Ñ–∞–π–ª
            update_excel_with_parsed_data(parsed_df)
            
            # –í—ã–∑—ã–≤–∞–µ–º callback –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω
            if callback:
                callback({
                    'success': True,
                    'brand': brand,
                    'model': model,
                    'parsed_parts': len(damaged_parts),
                    'found_prices': (parsed_df['—Ü–µ–Ω–∞'] > 0).sum(),
                    'dataframe': parsed_df
                })
                
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–æ—Ç–æ–∫–µ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            if callback:
                callback({
                    'success': False,
                    'error': str(e)
                })
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    thread = Thread(target=parsing_thread)
    thread.daemon = True
    thread.start()
    
    logging.info(f"üöÄ –ó–∞–ø—É—â–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è {brand} {model}")
    return thread
